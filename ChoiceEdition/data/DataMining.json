[
    {
        "correct": [
            "被错误地分为负类的样本"
        ],
        "options": [
            "被错误地分为正类的样本",
            "被正确地分为正类的样本",
            "被错误地分为负类的样本",
            "被正确地分为负类的样本"
        ],
        "count": 3,
        "question": "混淆矩阵中False Negative的含义是：\r"
    },
    {
        "correct": [
            "越靠上越好（AUC趋近于1）"
        ],
        "options": [
            "对角线（AUC等于0.5）",
            "越靠下越好（AUC趋近于0）",
            "越靠上越好（AUC趋近于1）"
        ],
        "count": 3,
        "question": "在ROC分析中，分类器的性能曲线的理想状态是：\r"
    },
    {
        "correct": [
            "银行信用卡评分模型"
        ],
        "options": [
            "区分猫狗图片",
            "社交网络好友推荐",
            "手写体识别",
            "银行信用卡评分模型"
        ],
        "count": 3,
        "question": "以下最有可能涉及代价敏感分类问题的是：\r"
    },
    {
        "correct": [
            "号码的纯随机性"
        ],
        "options": [
            "数据样本不够大",
            "号码的纯随机性",
            "现有模型不够复杂",
            "特征维度太高"
        ],
        "count": 3,
        "question": "彩票号码难以预测的原因在于：\r"
    },
    {
        "correct": [
            "然而并不能说明什么"
        ],
        "options": [
            "X增大会导致Y减小",
            "X减小会导致Y增大",
            "X增大不会导致Y增大",
            "然而并不能说明什么"
        ],
        "count": 3,
        "question": "两个变量X和Y呈现负相关性，说明：\r"
    },
    {
        "correct": [
            "对拥挤人群进行预警",
            "优化商场布局",
            "个性化营销"
        ],
        "options": [
            "对拥挤人群进行预警",
            "优化商场布局",
            "防盗",
            "个性化营销"
        ],
        "count": 3,
        "question": "在超市环境中对客户位置轨迹进行记录和分析的主要目的有哪些？\r"
    },
    {
        "correct": [
            "数据装载",
            "数据提取",
            "数据转换"
        ],
        "options": [
            "数据提取",
            "数据转换",
            "数据分析",
            "数据装载"
        ],
        "count": 3,
        "question": "ETL系统的主要作用包括："
    },
    {
        "correct": [
            "数据有无标签"
        ],
        "options": [
            "数据维度不同",
            "数据类型不同",
            "数据有无标签",
            "计算复杂度不同"
        ],
        "count": 3,
        "question": "聚类与分类的主要区别在于：\r"
    },
    {
        "correct": [
            "√"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "在实际数据分析工作中，数据类型转换和数据自身的错误是面临的主要挑战之一。\r"
    },
    {
        "correct": [
            "×"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "大数据和传统数据分析相比，核心特征就是数据量大。\r"
    },
    {
        "correct": [
            "×"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "分类器在训练样本上的学习误差越低越好。\r"
    },
    {
        "correct": [
            "×"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "短期股票价格波动难以精准预测的主要原因在于现有模型本身不够精密。\r"
    },
    {
        "correct": [
            "Inconsistent Data"
        ],
        "options": [
            "Missing Data",
            "Inconsistent Data",
            "Noisy Data",
            "Redundant Data"
        ],
        "count": 3,
        "question": "小张的个人信息中身份证号倒数第二位是单数，性别为女。这种情况被称为：\r"
    },
    {
        "correct": [
            "N/A"
        ],
        "options": [
            "完全随机缺失",
            "N/A",
            "数据未提供",
            "异常数据"
        ],
        "count": 3,
        "question": "学生小明在调查问卷中没有回答下述问题：“你去年的工资收入和前年相比是否有所增加？” 对这种情况最恰当的描述是：\r"
    },
    {
        "correct": [
            "具体问题具体分析"
        ],
        "options": [
            "删就一个字",
            "用均值填充即可",
            "用中位数填充即可",
            "具体问题具体分析"
        ],
        "count": 3,
        "question": "以下针对缺失值问题的阐述正确的是：\r"
    },
    {
        "correct": [
            "60公斤"
        ],
        "options": [
            "40公斤",
            "60公斤",
            "80公斤",
            "100公斤"
        ],
        "count": 3,
        "question": "某大一男生体检数据中体重值缺失，相对合理的填充值是：\r"
    },
    {
        "correct": [
            "可根据其它信息（如身高、体重）推测"
        ],
        "options": [
            "填1",
            "填0",
            "5，必须的",
            "可根据其它信息（如身高、体重）推测"
        ],
        "count": 3,
        "question": "假设男生用1表示，女生用0表示，某人的性别未填，应该如何处理？\r"
    },
    {
        "correct": [
            "不能简单判定"
        ],
        "options": [
            "一回事，说法不同而已",
            "离群点一定是异常点",
            "异常点一定是离群点",
            "不能简单判定"
        ],
        "count": 3,
        "question": "下关于离群点（Outlier）和异常点（Anomaly）关系的论述正确的是：\r"
    },
    {
        "correct": [
            "需要考虑相对距离因素"
        ],
        "options": [
            "主要看其与近邻的平均距离",
            "主要看其与近邻的最大距离",
            "需要考虑相对距离因素",
            "主要靠感觉"
        ],
        "count": 3,
        "question": "关于离群点的判定：\r"
    },
    {
        "correct": [
            "LOF值越大越疑似离群点"
        ],
        "options": [
            "LOF值越小越疑似离群点",
            "LOF值越大越疑似离群点",
            "LOF值越接近1越疑似离群点",
            "5越疑似离群点"
        ],
        "count": 3,
        "question": "采用LOF方法进行离群点检测时：\r"
    },
    {
        "correct": [
            "A为重复数据可能性大"
        ],
        "options": [
            "A为重复数据可能性大",
            "B为重复数据可能性大",
            "我读书少，看不出什么区别"
        ],
        "count": 3,
        "question": "Case A：两人名字不同，身份证号相同。 Case B：两人同名同姓，身份证号不同。"
    },
    {
        "correct": [
            "前三位不容易记错"
        ],
        "options": [
            "前三位不容易记错",
            "末尾三位不容易记错",
            "中间三位不容易记错",
            "都一样"
        ],
        "count": 3,
        "question": "在记录手机号码的时候，相对而言：\r"
    },
    {
        "correct": [
            "姓容易写错"
        ],
        "options": [
            "姓容易写错",
            "名容易写错",
            "没有明显区别"
        ],
        "count": 3,
        "question": "在记录英语国家人名时：\r"
    },
    {
        "correct": [
            "姓的区分度大"
        ],
        "options": [
            "姓的区分度大",
            "名的区分度大",
            "没有明显区别"
        ],
        "count": 3,
        "question": "对英语国家的人群而言：\r"
    },
    {
        "correct": [
            "序数型（Ordinal）"
        ],
        "options": [
            "数值型（连续）",
            "数值型（离散）",
            "序数型（Ordinal）",
            "标称型（Nominal）"
        ],
        "count": 3,
        "question": "按A, B, C, D打分的考试成绩数据属于：\r"
    },
    {
        "correct": [
            "不同编码可能会影响数据的空间分布"
        ],
        "options": [
            "顺序编码即可",
            "类别较少时，可考虑采用扩维法",
            "不同编码可能会影响数据的空间分布",
            "不好处理，删了算了"
        ],
        "count": 3,
        "question": "在对标称型数据（如颜色、职业等）进行编码时：\r"
    },
    {
        "correct": [
            "少数类样本的准确率"
        ],
        "options": [
            "整体的准确率",
            "多数类样本的准确率",
            "少数类样本的准确率",
            "两类样本准确率的均值"
        ],
        "count": 3,
        "question": "对于极度不平衡的二分类数据集，应特别注意：\r"
    },
    {
        "correct": [
            "个体收入分布极度不均衡"
        ],
        "options": [
            "自己工作不够努力，怨不得别人",
            "统计样本不具有代表性",
            "个体收入分布极度不均衡",
            "错觉，都是错觉"
        ],
        "count": 3,
        "question": "很多人感觉到自己的收入与官方公布的平均收入相去甚远，最有可能的解释是：\r"
    },
    {
        "correct": [
            "25%到75%之间的数据分布较为集中"
        ],
        "options": [
            "25%到75%之间的数据分布较为集中",
            "25%到75%之间的数据分布较为分散",
            "离群点较少",
            "离群点较多"
        ],
        "count": 3,
        "question": "在Box Plots当中，一个盒子越扁说明在该维度上：\r"
    },
    {
        "correct": [
            "平行坐标"
        ],
        "options": [
            "圆饼图",
            "散点图",
            "平行坐标",
            "直方图"
        ],
        "count": 3,
        "question": "适合可视化高维数据的方法是：\r"
    },
    {
        "correct": [
            "贯穿数据挖掘工作全过程"
        ],
        "options": [
            "锦上添花，可有可无",
            "不学就懂，一看就会",
            "主要用于展示最终结果",
            "贯穿数据挖掘工作全过程"
        ],
        "count": 3,
        "question": "数据可视化工作：\r"
    },
    {
        "correct": [
            "5"
        ],
        "options": [
            "9",
            "7",
            "5",
            "2"
        ],
        "count": 3,
        "question": "假设某数据集的原始熵值为0.7， 已知某属性的信息增益为0.2，那么利用该属性进行划分后数据集的熵值为：\r"
    },
    {
        "correct": [
            "特征提取包含特征选择"
        ],
        "options": [
            "特征提取包含特征选择",
            "特征选择包含特征提取",
            "一码事，说法不同而已",
            "It is like comparing apples and oranges."
        ],
        "count": 3,
        "question": "特征选择与特征提取的关系是：\r"
    },
    {
        "correct": [
            "需要借助领域知识",
            "数据挖掘工作的基础性工作"
        ],
        "options": [
            "需要借助领域知识",
            "核心内容就是缺失数据填充",
            "数据挖掘工作的基础性工作",
            "主要靠标准化算法自动处理"
        ],
        "count": 3,
        "question": "以下关于数据预处理的描述正确的是：\r"
    },
    {
        "correct": [
            "减少需要处理的数据量",
            "有助于处理不平衡数据",
            "提高数据的稳定性"
        ],
        "options": [
            "降低获取数据的成本",
            "减少需要处理的数据量",
            "有助于处理不平衡数据",
            "提高数据的稳定性"
        ],
        "count": 3,
        "question": "在大数据分析中，利用采样技术可以：\r"
    },
    {
        "correct": [
            "X和Y是否正相关",
            "X和Y是否负相关"
        ],
        "options": [
            "X和Y是否正相关",
            "X和Y是否负相关",
            "X和Y是否不相关",
            "X和Y之间的因果关系"
        ],
        "count": 3,
        "question": "Pearson’s product moment correlation coefficient 可用来判断：\r"
    },
    {
        "correct": [
            "属性可能存在冗余",
            "属性可能存在噪声",
            "降低问题复杂度"
        ],
        "options": [
            "属性可能存在冗余",
            "属性可能存在噪声",
            "降低问题复杂度",
            "个人喜好"
        ],
        "count": 3,
        "question": "进行属性选择的原因是：\r"
    },
    {
        "correct": [
            "×"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "熵衡量的是系统的不确定性，熵值越大（接近于1）说明系统的不确定性越低。\r"
    },
    {
        "correct": [
            "得具体问题具体分析"
        ],
        "options": [
            "《瓦尔登湖》的读者很有可能会买《理想国》",
            "《理想国》的读者很有可能会买《瓦尔登湖》",
            "两本书的读者都很有可能买另一本书",
            "得具体问题具体分析"
        ],
        "count": 3,
        "question": "已知梭罗的《瓦尔登湖》和柏拉图的《理想国》经常被同时购买，那么：\r"
    },
    {
        "correct": [
            "向上销售"
        ],
        "options": [
            "交叉销售",
            "向上销售",
            "捆绑销售",
            "被坑了"
        ],
        "count": 3,
        "question": "某人买电脑的预算为6000元，最终从电脑城买了一台8000元的电脑，学术上的解释是：\r"
    },
    {
        "correct": [
            "交叉销售"
        ],
        "options": [
            "交叉销售",
            "向上销售",
            "捆绑销售",
            "女人啊"
        ],
        "count": 3,
        "question": "某人望着一柜子衣服，感觉自己没有衣服穿，遂上街四个小时，购得手袋一只、高跟鞋若干双、帽子一顶、丝巾一条、YSL口红一只...... 针对以上行为，学术上的解释是：\r"
    },
    {
        "correct": [
            "香水、化妆品"
        ],
        "options": [
            "女装",
            "男装",
            "美食广场",
            "香水、化妆品"
        ],
        "count": 3,
        "question": "百货商场第一层进门区域通常会布置为：\r"
    },
    {
        "correct": [
            "还记得那些年我们一起学过的条件独立吗？"
        ],
        "options": [
            "冰激凌含有某种神秘物质能够诱发犯罪",
            "从事违法行为后，人们喜欢吃冰激凌平静一下心情",
            "还记得那些年我们一起学过的条件独立吗？",
            "我的智商该充值了"
        ],
        "count": 3,
        "question": "冰激凌和犯罪的例子说明：\r"
    },
    {
        "correct": [
            "候选项集总数过于庞大"
        ],
        "options": [
            "每条交易记录可能很长",
            "数据库可能包含很多条交易记录",
            "候选项集总数过于庞大",
            "存储器I/O读写耗时"
        ],
        "count": 3,
        "question": "用蛮力搜索所有的频繁项集的最大困难在于：\r"
    },
    {
        "correct": [
            "最终的输出结果是长度最长的频繁项集"
        ],
        "options": [
            "所有频繁项集的子集都是频繁的",
            "所有不频繁项集的超集都是不频繁的",
            "对频繁项集的搜索遵循bottom-up的原则",
            "最终的输出结果是长度最长的频繁项集"
        ],
        "count": 3,
        "question": "关于Apriori算法说法不正确的是：\r"
    },
    {
        "correct": [
            "空间剪枝"
        ],
        "options": [
            "空间剪枝",
            "启发式搜索",
            "折半查找",
            "分支定界"
        ],
        "count": 3,
        "question": "对Apriori算法工作原理最贴切的说法是：\r"
    },
    {
        "correct": [
            "所有可能频繁的K+1项集都在Ck+1中"
        ],
        "options": [
            "所有可能频繁的K+1项集都在Lk+1中",
            "尽可能多的K+1项集都在Ck+1中",
            "所有可能频繁的K+1项集都在Ck+1中",
            "我有点乱"
        ],
        "count": 3,
        "question": "在Apriori算法中，假设已获得Lk，则寻找K+1频繁项集时应确保：\r"
    },
    {
        "correct": [
            "已购买小米5, 推荐手机贴膜"
        ],
        "options": [
            "已购买iPhone 6，推荐iPhone 6 Plus",
            "已购买小米5, 推荐手机贴膜",
            "已购买华为Mate 7, 推荐备用电池",
            "已购买Galaxy Note 7, 推荐防爆套装"
        ],
        "count": 3,
        "question": "以下哪种推荐最为靠谱：\r"
    },
    {
        "correct": [
            "完全相同的项集不能在同一序列中重复出现"
        ],
        "options": [
            "序列中包含项集",
            "完全相同的项集不能在同一序列中重复出现",
            "序列强调时间上的先后顺序",
            "不同序列对应不同的客户ID"
        ],
        "count": 3,
        "question": "以下关于序列和项集说法不正确的是：\r"
    },
    {
        "correct": [
            "<{B} {A, A} {C}>"
        ],
        "options": [
            "<{A, B, C}>",
            "<{A} {A} {C}>",
            "<{B} {A, A} {C}>",
            "<{A, B} {A, B, C}>"
        ],
        "count": 3,
        "question": "给定A，B，C三件商品，以下哪条序列不正确：\r"
    },
    {
        "correct": [
            "<{2} {3} {5}>"
        ],
        "options": [
            "<{2} {8}>",
            "<{2} {3, 6}>",
            "<{3, 5} {8}>",
            "<{2} {3} {5}>"
        ],
        "count": 3,
        "question": "已知序列<{2, 3} {3, 6, 5} {8}>，以下哪条序列不是其子序列：\r"
    },
    {
        "correct": [
            "支持度高：足够频繁",
            "置信度高：足够有说服力",
            "前件和后件交集为空"
        ],
        "options": [
            "支持度高：足够频繁",
            "置信度高：足够有说服力",
            "前件和后件交集为空",
            "前件和后件必须包含多个item"
        ],
        "count": 3,
        "question": "一条有价值的关联规则必须满足：\r"
    },
    {
        "correct": [
            "√"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "关联规则X->Y的支持度等同于{X, Y}的支持度。\r"
    },
    {
        "correct": [
            "√"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "关联规则X->Y的置信度等价于条件概率P(Y|X)的值。\r"
    },
    {
        "correct": [
            "×"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "关联规则X->Y是一条强规则指的是{X, Y}在数据库中频繁出现。\r"
    },
    {
        "correct": [
            "×"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "一条关联规则的置信度只需大于预设的阈值就是有价值的规则。\r"
    },
    {
        "correct": [
            "×"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "只要关联规则X->Y的置信度大于Y自身的概率就是一条有价值的关联规则。\r"
    },
    {
        "correct": [
            "×"
        ],
        "options": [
            "√",
            "×"
        ],
        "count": 3,
        "question": "因为关联规则描述的是事件之间的条件概率，因此可以用于推断因果关系。\r"
    },
    {
        "correct": [
            "学习样本是否需要人工标记"
        ],
        "options": [
            "学习过程是否需要人工干预",
            "学习样本是否需要人工标记",
            "学习结果是否需要人工解释",
            "学习参数是否需要人工设置"
        ],
        "count": 3,
        "question": "有监督的学习和无监督的学习的根本区别在于：\r"
    },
    {
        "correct": [
            "40%"
        ],
        "options": [
            "20%",
            "30%",
            "40%",
            "50%"
        ],
        "count": 3,
        "question": "已知池中有两种鱼，比例为7:3，若随机捞上一条，按照70%和30%概率随机猜测其种类，则整体误差最接近于：\r"
    },
    {
        "correct": [
            "1/3"
        ],
        "options": [
            "1/2",
            "1/3",
            "1/4",
            "真的不关我的事"
        ],
        "count": 3,
        "question": ""
    },
    {
        "correct": [
            "95"
        ],
        "options": [
            "85",
            "90",
            "95",
            "00"
        ],
        "count": 3,
        "question": "已知甲乙丙三人射击命中率分别为0.8，0.6和0.5，若每人各开一枪，则目标被命中的概率最接近：\r"
    },
    {
        "correct": [
            "及时复检，防止假阳性"
        ],
        "options": [
            "心如死灰，万念俱灭",
            "散尽家财，及时行乐",
            "置若罔闻，我行我素",
            "及时复检，防止假阳性"
        ],
        "count": 3,
        "question": "当化验报告呈阳性的时候，正确的做法是：\r"
    },
    {
        "correct": [
            "属性之间的条件独立性假设"
        ],
        "options": [
            "只能处理低维属性",
            "只能处理离散型属性",
            "分类效果一般",
            "属性之间的条件独立性假设"
        ],
        "count": 3,
        "question": "朴素贝叶斯分类器的朴素之处在于：\r"
    },
    {
        "correct": [
            "若独立一定不相关"
        ],
        "options": [
            "若独立一定不相关",
            "若不相关一定独立",
            "若独立不一定不相关",
            "我已经晕了"
        ],
        "count": 3,
        "question": "以下关于两个变量X和Y说法正确的是：\r"
    },
    {
        "correct": [
            "P(A|B, C)=P(A|C)"
        ],
        "options": [
            "P(A, B)=P(A)P(B)",
            "P(A, B)=P(A|B)P(B)",
            "P(A|B, C)=P(A|C)",
            "P(A|B)=P(A)"
        ],
        "count": 3,
        "question": "两个事件A和B条件独立指的是：\r"
    },
    {
        "correct": [
            "防止计算条件概率时分子为零"
        ],
        "options": [
            "防止计算条件概率时分母为零",
            "防止计算条件概率时分子为零",
            "用于解决训练集中的噪声",
            "用于解决训练集中的异常值"
        ],
        "count": 3,
        "question": "以下关于拉普拉斯平滑说法正确的是：\r"
    },
    {
        "correct": [
            "一个单词可能存在于多个词袋中但频率不同"
        ],
        "options": [
            "任何一个单词只能存在于某一个词袋中",
            "一个单词可能存在于多个词袋中但频率不同",
            "所有词袋中单词的并集就等同于词汇表",
            "词袋模型描述的是单词在所有文本中出现的频率"
        ],
        "count": 3,
        "question": "在文本分类应用中，关于词袋模型的描述正确的是：\r"
    },
    {
        "correct": [
            "可解释性好"
        ],
        "options": [
            "训练时间短",
            "可解释性好",
            "善于处理缺失值",
            "鲁棒性好"
        ],
        "count": 3,
        "question": "作为一种分类器，决策树模型的主要优点是：\r"
    },
    {
        "correct": [
            "在训练集上A优于B，在测试集上B优于A"
        ],
        "options": [
            "在训练集上A优于B，在测试集上A也优于B",
            "在训练集上A优于B，在测试集上B优于A",
            "相对于分类数据集，决策树过于简单",
            "在训练集上决策树的误差很小"
        ],
        "count": 3,
        "question": "下列哪一种情况被称为过学习现象：\r"
    },
    {
        "correct": [
            "可以被使用多次"
        ],
        "options": [
            "必须被使用",
            "只能被使用一次",
            "可以被使用多次",
            "可以在任意位置被使用多次"
        ],
        "count": 3,
        "question": "任何一个候选属性在生成的决策树中：\r"
    },
    {
        "correct": [
            "对于某一个数据集，可以生成多个决策树"
        ],
        "options": [
            "决策树越复杂，分类能力越强",
            "在性能相同的情况下，通常选择能充分利用各种属性的决策树",
            "对于某一个数据集，只有一个决策树可以将其完美分开",
            "对于某一个数据集，可以生成多个决策树"
        ],
        "count": 3,
        "question": "以下关于决策树的说法正确的是：\r"
    },
    {
        "correct": [
            "容易造成过学习"
        ],
        "options": [
            "星座信息更有说服力",
            "容易造成过学习",
            "可能的取值太多，计算量过大",
            "两个人可能生日相同"
        ],
        "count": 3,
        "question": "为什么一般不推荐在决策树中使用“生日”属性：\r"
    },
    {
        "correct": [
            "信息增益大的属性应放在上层"
        ],
        "options": [
            "取值多的属性应放在上层",
            "取值少的属性应放在上层",
            "信息增益大的属性应放在上层",
            "应利用尽可能多的属性"
        ],
        "count": 3,
        "question": "决策树模型中建树的基本原则是：\r"
    },
    {
        "correct": [
            "从叶节点开始"
        ],
        "options": [
            "从中间节点开始",
            "从叶节点开始",
            "有助于保持树的平衡",
            "可以有效降低训练误差"
        ],
        "count": 3,
        "question": "关于决策树剪枝操作正确的描述是：\r"
    },
    {
        "correct": [
            "用于控制对模型的剪枝操作"
        ],
        "options": [
            "用于校验模型的训练误差",
            "用于校验模型的测试误差",
            "用于校验模型的正确性",
            "用于控制对模型的剪枝操作"
        ],
        "count": 3,
        "question": "在决策树模型中，测试集的用途是：\r"
    },
    {
        "correct": [
            "根据信息增益选择阈值进行离散化"
        ],
        "options": [
            "直接忽略",
            "利用固定阈值进行离散化",
            "根据信息增益选择阈值进行离散化",
            "随机选择数据标签发生变化的位置进行离散化"
        ],
        "count": 3,
        "question": "决策树模型中应如何妥善处理连续型属性：\r"
    },
    {
        "correct": [
            "Entities are not to be multiplied beyond necessity.",
            "Among competing hypotheses, the one with the fewest assumptions should be selected.",
            "The simplest explanation is usually the correct one."
        ],
        "options": [
            "Entities are not to be multiplied beyond necessity.",
            "Among competing hypotheses, the one with the fewest assumptions should be selected.",
            "The simplest explanation is usually the correct one.",
            "中世纪英国上流社会的一种生活用品。"
        ],
        "count": 3,
        "question": "奥卡姆的剃刀指的是：\r"
    },
    {
        "correct": [
            "当前数据子集的标签一致",
            "没有更多可用属性",
            "当前数据子集为空"
        ],
        "options": [
            "当前数据子集的标签一致",
            "没有更多可用属性",
            "当前数据子集为空",
            "当前训练误差已经较低"
        ],
        "count": 3,
        "question": "哪些情况下必须停止树的增长：\r"
    },
    {
        "correct": [
            "神经元的大规模分布式信息存储机制"
        ],
        "options": [
            "记性好，没办法",
            "刻骨铭心，矢志不渝",
            "天长地久有时尽，此情绵绵无绝期",
            "神经元的大规模分布式信息存储机制"
        ],
        "count": 3,
        "question": "我们很难刻意忘掉一个人的原因是：\r"
    },
    {
        "correct": [
            "与门"
        ],
        "options": [
            "或门",
            "与门",
            "非门",
            "与非门"
        ],
        "count": 3,
        "question": "如图所示的感知机（阈值为0）实现的逻辑功能是：\r"
    },
    {
        "correct": [
            "控制判决平面到原点的距离"
        ],
        "options": [
            "为了后续学习算法推导的方便",
            "其实在实际中可以略去",
            "控制判决平面到原点的距离",
            "控制判决平面的方向"
        ],
        "count": 3,
        "question": "在感知机的判决函数中，w0的作用是：\r"
    },
    {
        "correct": [
            "权重应减小"
        ],
        "options": [
            "权重应增加",
            "权重应减小",
            "不能确定"
        ],
        "count": 3,
        "question": "若神经元的误差对某输入的权重的偏导大于零说明：\r"
    },
    {
        "correct": [
            "感知机的训练过程可以看成是在误差空间进行梯度下降"
        ],
        "options": [
            "在batch learning模式下，权重调整出现在学习每个样本之后",
            "只要参数设置得当，感知机理论上可以解决各种分类问题",
            "感知机的训练过程可以看成是在误差空间进行梯度下降",
            "感知机的激励函数必须采用门限函数"
        ],
        "count": 3,
        "question": "以下关于感知机说法正确的是：\r"
    },
    {
        "correct": [
            "感知机只能形成线性判决平面，无法解决异或问题"
        ],
        "options": [
            "多层感知机比感知机只多了一个隐含层",
            "感知机只能形成线性判决平面，无法解决异或问题",
            "多层感知机可以有多个隐含层，但是只能有一个输出单元",
            "隐含层神经元的个数应当小于输入层神经元的个数"
        ],
        "count": 3,
        "question": "以下关于感知机说法正确的是：\r"
    },
    {
        "correct": [
            "将原始问题在隐含层映射成线性可分问题"
        ],
        "options": [
            "分而治之，对原始问题空间进行划分",
            "将原始问题向更高维空间映射",
            "在输出层和隐含层之间形成非线性的分界面",
            "将原始问题在隐含层映射成线性可分问题"
        ],
        "count": 3,
        "question": "多层感知机解决线性不可分问题的原理是：\r"
    },
    {
        "correct": [
            "遵循相同的原理，激励函数可能有所不同"
        ],
        "options": [
            "考虑到线性不可分问题，学习规则更为复杂",
            "一模一样，等价于多个感知机",
            "遵循相同的原理，激励函数可能有所不同",
            "所有输出层神经元的权重需要同步调整"
        ],
        "count": 3,
        "question": "在误差逆传播算法中，输出层神经元权重的调整机制和感知机的学习规则相比：\r"
    },
    {
        "correct": [
            "根据自身下游神经元的误差进行加权计算"
        ],
        "options": [
            "根据自身的期望输出和实际输出的差值计算",
            "根据所有输出层神经元的误差的均值计算",
            "根据自身下游神经元的误差进行加权计算",
            "根据自身下游神经元的误差的均值计算"
        ],
        "count": 3,
        "question": "在误差逆传播算法中，隐含层节点的误差信息应当：\r"
    },
    {
        "correct": [
            "尝试从不同的初始点开始训练"
        ],
        "options": [
            "尝试从不同的初始点开始训练",
            "将权重初始化为接近于0的值",
            "采用较小的学习率",
            "增加隐含层神经元个数"
        ],
        "count": 3,
        "question": "为了克服学习空间中存在的局部最优点应当：\r"
    },
    {
        "correct": [
            "在开始阶段应该较大，然后逐渐减小"
        ],
        "options": [
            "较大的值有助于提高算法的收敛稳定性",
            "较小的值有助于提高算法的收敛速度",
            "在开始阶段应该较大，然后逐渐减小",
            "在开始阶段应该较小，然后逐渐增大"
        ],
        "count": 3,
        "question": "关于学习率参数的设置，正确的描述是：\r"
    },
    {
        "correct": [
            "有助于摆脱误差平缓区域"
        ],
        "options": [
            "提高算法的收敛精度",
            "提高算法的稳健性",
            "提高算法的全局优化能力",
            "有助于摆脱误差平缓区域"
        ],
        "count": 3,
        "question": "在权重更新公式中引入冲量的主要目的是：\r"
    },
    {
        "correct": [
            "当前的网络输入和第T-1时刻网络的内部状态"
        ],
        "options": [
            "当前的网络输入",
            "当前的网络输入和第T-1时刻网络的内部状态",
            "第T-1时刻网络的内部状态",
            "当前的网络输入和第1到T-1时刻网络的内部状态"
        ],
        "count": 3,
        "question": "在Elman网络中，第T时刻网络的输出取决于：\r"
    },
    {
        "correct": [
            "有固定的输出上下界",
            "导数存在解析解",
            "处处可导"
        ],
        "options": [
            "有固定的输出上下界",
            "计算复杂度较低",
            "导数存在解析解",
            "处处可导"
        ],
        "count": 3,
        "question": "采用Sigmod函数作为激励函数的主要原因是：\r"
    },
    {
        "correct": [
            "基于内容的检索",
            "联想记忆功能",
            "含噪声的模式识别"
        ],
        "options": [
            "基于内容的检索",
            "联想记忆功能",
            "误差逆传播",
            "含噪声的模式识别"
        ],
        "count": 3,
        "question": "以下关于Hopfield网络特性的描述正确的是：\r"
    },
    {
        "correct": [
            "训练样本含有噪声",
            "需要较快的测试响应速度",
            "多分类问题"
        ],
        "options": [
            "训练时间有限",
            "训练样本含有噪声",
            "需要较快的测试响应速度",
            "较好的可解释性",
            "多分类问题"
        ],
        "count": 3,
        "question": "前馈神经网络适用的场景为：\r"
    },
    {
        "correct": [
            "间隔"
        ],
        "options": [
            "盈利率",
            "马金",
            "间隔",
            "保证金"
        ],
        "count": 3,
        "question": "在SVM领域中，margin的含义是：\r"
    },
    {
        "correct": [
            "是否确保间隔最大化"
        ],
        "options": [
            "是否进行了空间映射",
            "是否确保间隔最大化",
            "是否能处理线性不可分问题",
            "训练误差通常较低"
        ],
        "count": 3,
        "question": "线性SVM和一般线性分类器的区别主要是：\r"
    },
    {
        "correct": [
            "有望获得较低的测试误差"
        ],
        "options": [
            "所需的支持向量个数最少",
            "计算复杂度最低",
            "训练误差最",
            "有望获得较低的测试误差"
        ],
        "count": 3,
        "question": "为什么通常要选择margin最大的分类器？\r"
    },
    {
        "correct": [
            "2/|w|"
        ],
        "options": [
            "1/|w|",
            "2/|w|",
            "|b|/|w|",
            "2|b|/|W|"
        ],
        "count": 3,
        "question": "假设超平面为w*x+b=0，其margin的大小为：\r"
    },
    {
        "correct": [
            "决定分类面可以平移的范围的数据点"
        ],
        "options": [
            "对原始数据进行采样得到的样本点",
            "决定分类面可以平移的范围的数据点",
            "位于分类面上的点",
            "能够被正确分类的数据点"
        ],
        "count": 3,
        "question": "支持向量（support vectors）指的是：\r"
    },
    {
        "correct": [
            "alpha>0的数据点是支持向量"
        ],
        "options": [
            "alpha=0的数据点是支持向量",
            "alpha>0的数据点是支持向量",
            "alpha<0的数据点是支持向量",
            "两者没有固定关系"
        ],
        "count": 3,
        "question": "在SVM的求解过程中，支持向量与alpha的关系是：\r"
    },
    {
        "correct": [
            "向量内积"
        ],
        "options": [
            "向量内积",
            "矩阵乘法",
            "矩阵转置",
            "矩阵分解"
        ],
        "count": 3,
        "question": "在SVM当中，主要的运算形式是：\r"
    },
    {
        "correct": [
            "解决不完全线性可分问题"
        ],
        "options": [
            "解决线性不可分问题",
            "解决不完全线性可分问题",
            "降低算法时间复杂度",
            "提高算法分类精确"
        ],
        "count": 3,
        "question": "软间隔（soft margin）的主要用途是：\r"
    },
    {
        "correct": [
            "提高原始问题的可分性"
        ],
        "options": [
            "降低计算复杂度",
            "提取较为重要的特征",
            "对原始数据进行标准化",
            "提高原始问题的可分性"
        ],
        "count": 3,
        "question": "在SVM当中进行空间映射的主要目的是：\r"
    },
    {
        "correct": [
            "计算复杂度高"
        ],
        "options": [
            "模型可解释性差",
            "计算复杂度高",
            "容易出现奇异矩阵",
            "容易出现稀疏矩阵"
        ],
        "count": 3,
        "question": "对于SVM，在映射后的高维空间直接进行计算的主要问题是：\r"
    },
    {
        "correct": [
            "利用在原始空间定义的函数替代高维空间的向量内积操作"
        ],
        "options": [
            "利用在原始空间定义的函数替代高维空间的向量内积操作",
            "利用在高维空间定义的函数替代原始空间的向量内积操作",
            "核函数的导数具有简单的解析解，简化了运算",
            "核函数具有固定的上下界，可以输出（-1, +1）区间中的连续值"
        ],
        "count": 3,
        "question": "所谓kernel trick，指的是：\r"
    },
    {
        "correct": [
            "避免高维空间运算，降低算法复杂度"
        ],
        "options": [
            "提高算法的可解释性",
            "生成数量较少的支持向量",
            "生成数量较多的支持向量",
            "避免高维空间运算，降低算法复杂度"
        ],
        "count": 3,
        "question": "通过运用核函数，我们可以：\r"
    },
    {
        "correct": [
            "不知道在哪儿"
        ],
        "options": [
            "上幼儿园",
            "上小学",
            "上中学",
            "不知道在哪儿"
        ],
        "count": 3,
        "question": "线性SVM思想最初被提出的时候，你在：\r"
    },
    {
        "correct": [
            "惊天引用次数",
            "支持向量机开天辟地",
            "统计学习理论一代宗师",
            "目光如炬，深不可测"
        ],
        "options": [
            "惊天引用次数",
            "支持向量机开天辟地",
            "统计学习理论一代宗师",
            "目光如炬，深不可测"
        ],
        "count": 3,
        "question": "Владимир Наумович Вапник（Vladimir Vapnik）为什么是真神：\r"
    },
    {
        "correct": [
            "不是一码事，但实际中有一定联系"
        ],
        "options": [
            "簇即是类、类即是簇",
            "簇是类的一种具体表现形式",
            "类是簇的一种具体表现形式",
            "不是一码事，但实际中有一定联系"
        ],
        "count": 3,
        "question": "聚类中的簇（cluster）与分类中的类（class）的关系是：\r"
    },
    {
        "correct": [
            "对客户群进行划分"
        ],
        "options": [
            "对客户群进行划分",
            "进行商品推荐",
            "识别优质客户",
            "辅助商品定价"
        ],
        "count": 3,
        "question": "在市场营销中，聚类最有可能帮助经营者：\r"
    },
    {
        "correct": [
            "有助于提升聚类质量"
        ],
        "options": [
            "可能改变数据点之间的位置关系",
            "可能改变簇的个数",
            "有助于提升聚类质量",
            "可能产生不确定影响"
        ],
        "count": 3,
        "question": "关于数据预处理对聚类分析的影响的错误说法是：\r"
    },
    {
        "correct": [
            "簇的个数越多，分割后图像越接近原始图像"
        ],
        "options": [
            "色彩越复杂的图，需要的簇的个数越少",
            "属于同一个物体的像素对应同一个簇",
            "簇的个数越少，分割后图像越接近原始图像",
            "簇的个数越多，分割后图像越接近原始图像"
        ],
        "count": 3,
        "question": "在基于聚类的图像分割例子中：\r"
    },
    {
        "correct": [
            "需要考虑数据点间的连通性"
        ],
        "options": [
            "簇内数据点散布越小越好",
            "簇中心点之间的距离越大越好",
            "簇的个数越小越好",
            "需要考虑数据点间的连通性"
        ],
        "count": 3,
        "question": "如何衡量聚类的质量：\r"
    },
    {
        "correct": [
            "可以体现出簇的紧凑性"
        ],
        "options": [
            "每个点的取值范围为[0, 1]",
            "每个点的取值越接近于0越好",
            "可以体现出簇的紧凑性",
            "对于离群点，取值可能超过1"
        ],
        "count": 3,
        "question": "对于Silhouette图表述正确的是：\r"
    },
    {
        "correct": [
            "对初始中心点较为敏感"
        ],
        "options": [
            "对数据分布没有特殊的要求",
            "能较好处理噪点和离群点",
            "对初始中心点较为敏感",
            "计算复杂度较高"
        ],
        "count": 3,
        "question": "关于K-Means算法的表述正确的是：\r"
    },
    {
        "correct": [
            "直接影响算法的收敛结果"
        ],
        "options": [
            "可随意设置",
            "必须在每个簇的真实中心点的附近",
            "必须足够分散",
            "直接影响算法的收敛结果"
        ],
        "count": 3,
        "question": "K-Means算法中的初始中心点：\r"
    },
    {
        "correct": [
            "聚类结果可能受数据访问顺序影响"
        ],
        "options": [
            "需对数据集进行多次遍历",
            "无法人为控制最终聚类的个数",
            "需要事先生成初始中心点",
            "聚类结果可能受数据访问顺序影响"
        ],
        "count": 3,
        "question": "在Sequential Leader算法中：\r"
    },
    {
        "correct": [
            "对数据分布有更好的描述性"
        ],
        "options": [
            "有更高的精确度",
            "有更低的计算复杂度",
            "有更好的鲁棒性",
            "对数据分布有更好的描述性"
        ],
        "count": 3,
        "question": "在混合高斯模型中，每一个数据点：\r"
    },
    {
        "correct": [
            "可以被任一高斯生成但概率可能不等"
        ],
        "options": [
            "只能被某一个高斯生成",
            "可以被所有高斯等概率生成",
            "可以被任一高斯生成但概率可能不等",
            "可以被任一高斯生成且概率由高斯的权重决定"
        ],
        "count": 3,
        "question": "在混合高斯模型中，每一个数据点：\r"
    },
    {
        "correct": [
            "相加必须等于1"
        ],
        "options": [
            "可以为负值",
            "相加必须等于0",
            "相加必须等于1",
            "须由用户预先设定"
        ],
        "count": 3,
        "question": "在混合高斯模型中，每个高斯的权重：\r"
    },
    {
        "correct": [
            "隐含参数指的是每个数据点的簇标号"
        ],
        "options": [
            "模型参数指的是每个数据点的簇标号",
            "隐含参数指的是每个数据点的簇标号",
            "模型参数指的是簇的个数（即K值）",
            "隐含参数指的是簇中心点坐标"
        ],
        "count": 3,
        "question": "以K-Means算法为例，期望最大化算法中的：\r"
    },
    {
        "correct": [
            "每组实验中选择的硬币"
        ],
        "options": [
            "每组实验中正面朝上的次数",
            "每组实验中选择的硬币",
            "每枚硬币正面朝上的概率",
            "每枚硬币被选中的次数"
        ],
        "count": 3,
        "question": "在掷硬币的例子中，期望最大化算法的隐含参数指的是：\r"
    },
    {
        "correct": [
            "较低的计算复杂度"
        ],
        "options": [
            "能妥善处理噪点和离群点",
            "能处理不规则的数据分布",
            "不需要预先设定簇的个数",
            "较低的计算复杂度"
        ],
        "count": 3,
        "question": "与K-Means相比，基于密度的DBSCAN的优点不包括：\r"
    },
    {
        "correct": [
            "中心点"
        ],
        "options": [
            "中心点",
            "核心点",
            "边缘点",
            "噪点"
        ],
        "count": 3,
        "question": "在DBSCAN中，对数据点类型的划分中不包括：\r"
    },
    {
        "correct": [
            "直接无视"
        ],
        "options": [
            "划分到最近的簇",
            "所有噪点单独形成一个簇",
            "直接无视",
            "不做特别区分"
        ],
        "count": 3,
        "question": "在DBSCAN中，对于噪点：\r"
    },
    {
        "correct": [
            "对于N个数据点，可生成1到N个簇"
        ],
        "options": [
            "需要用户预先设定聚类的个数",
            "需要用户预先设定聚类个数的范围",
            "对于N个数据点，可生成1到N个簇",
            "对于N个数据点，可生成1到N/2个簇"
        ],
        "count": 3,
        "question": "在层次型聚类中：\r"
    },
    {
        "correct": [
            "由点集间随机的一对点的距离决定"
        ],
        "options": [
            "由点集间距离最近的一对点的距离决定",
            "由点集间距离最远的一对点的距离决定",
            "由点集间随机的一对点的距离决定",
            "由点集间所有点的平均距离决定"
        ],
        "count": 3,
        "question": "在层次型聚类中，两个点集之间的距离计算方法通常不包括：\r"
    },
    {
        "correct": [
            "能够处理非球形的数据分布",
            "能够处理噪点和离群点",
            "对样本输入序列不敏感",
            "对海量数据的可扩展性"
        ],
        "options": [
            "能够处理非球形的数据分布",
            "能够处理噪点和离群点",
            "对样本输入序列不敏感",
            "对海量数据的可扩展性"
        ],
        "count": 3,
        "question": "一个好的聚类算法应当具备哪些潜质：\r"
    }
]